---
title: "Introduction to AutoxgboostMC"
author: "Florian Pfisterer"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to AutoxgboostMC}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


```{r}
library(devtools)
load_all()

# Split into train and test data set.
train.idx = sample(seq_len(768), 506)
test.idx = setdiff(seq_len(768), train.idx)
train.task = subsetTask(pid.task, train.idx)
test.task = subsetTask(pid.task, test.idx)
```


## Scenario 1: Optimizing a single mesure (Area under the Curve) for a given task.

This scenario reflects the traditional use-case in AutoML scenarios.
Given a dataset we want to obtain a model that optimizes a given performance measure.
Additionally, we might want the process to stop at some point, so that the user
can investigate results and make changes to the search process.

We instantiate a new AutoxgboostMC object with a dataset (Task) and a list of measures.
In this case we choose auc as a measure.
Afterwards we call the `.$fit` method with a time-budget of $15$ seconds.
This runs the AutoML process.

```{r}
axgb = AutoxgboostMC$new(train.task, measures = list(auc))
axgb$fit(time.budget = 15L)
```

If we are not satisfied, we can continue the search process for more iterations:

```{r}
axgb$fit(time.budget = 5L)
```

And then use the resulting model to predcit on our test data.

```{r}
axgb$predict(test.task)
```


```{r}
axgb$plot_parallel_coordinates()
```

## Scenario 2: Optimizing Accuracy and Prediction Speed

```{r}
axgb = AutoxgboostMC$new(train.task, measures = list(auc, timepredict))
axgb$fit(time.budget = 10L)
```

And visualize: 

```{r}
axgb$plot_pareto_front()
axgb$plot_results()
```

## Scenario 3: Predictive Accuracy and Fairness

```{r}
age_fairf1 = setMeasurePars(fairf1, grouping = function(df) as.factor(df$age > 30))
axgb = AutoxgboostMC$new(train.task, measures = list(acc, age_fairf1))
axgb$set_tune_threshold(FALSE)
axgb$fit(time.budget = 20L)
p = axgb$predict(test.task)
```





## Scenario 4: Interpretability and Predictive Accuracy

```{r}
axgb = AutoxgboostMC$new(measures = list(auc, timepredict))
axgb$fit(pid.task, time.budget = 5L)
p = axgb$predict(iris.task)

axgb$optim.result$pareto.front
axgb$optim.result$pareto.set
axgb$optim.result$pareto.inds
```

## Scenario 5: Optimizing a subjective measure.

In this scenario, we aim to find a model, that optimizes a measure we can not compute directly from the data, as it heavily depends of our current judgement of the proposed models. This might be a measure that can for example be described as follows:
"I want a model that achieves a very low false positive rate. It needs to be interpretable and fair with respect to a certain characteristic (i.e. the variable race).
Additionally, after looking at diagnosis plots, I can determine, that the relationships my model learns do correspond to the true underlying process,and thus provide a numerical rating for each model."

As a user usually can not look at all intermediate results, we might also want to learn user preferences from a few ratings given by the user and use this data to extrapolate to unrated models, only querying the user for new ratings once in a while.


## UI Design Speccs

Tab1: AutoML Process:
Choose a budget, start the process, visualize progress wrt. some measures

Tab2: Pareto Front

Tab 3: Dataset overview




Architecture:


App: Starts / Controls and Visualizes results
