---
title: "Introduction to AutoxgboostMC"
author: "Florian Pfisterer"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to AutoxgboostMC}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


```{r}
library(devtools)
load_all()

# Split into train and test data set.
train.idx = sample(seq_len(768), 506)
test.idx = setdiff(seq_len(768), train.idx)
train.task = subsetTask(pid.task, train.idx)
test.task = subsetTask(pid.task, test.idx)
```


## Scenario 1: Optimizing a single mesure (Area under the Curve) for a given task.

This scenario reflects the traditional use-case in AutoML scenarios.
Given a dataset we want to obtain a model that optimizes a given performance measure.
Additionally, we might want the process to stop at some point, so that the user
can investigate results and make changes to the search process.

We instantiate a new AutoxgboostMC object with a dataset (Task) and a list of measures.
In this case we choose auc as a measure.
Afterwards we call the `.$fit` method with a time-budget of $15$ seconds.
This runs the AutoML process.

```{r}
axgb = AutoxgboostMC$new(train.task, measures = list(auc))
axgb$fit(time_budget = 15L)
```

We can print the result:
```{r}
axgb
```


If we are not satisfied, we can continue the search process for more iterations:

```{r}
axgb$fit(time_budget = 5L)
```

And then use the resulting model to predcit on our test data.

```{r}
axgb$predict(test.task)
```


```{r}
axgb$plot_parallel_coordinates()
```



## Scenario 2: Optimizing Accuracy and Prediction Speed

```{r}
axgb = AutoxgboostMC$new(train.task, measures = list(auc, timepredict))
axgb$fit(time_budget = 15L)
```

And visualize:

```{r}
axgb$plot_pareto_front()
axgb$plot_parallel_coordinates()
axgb$plot_opt_result()
```

## Scenario 3: Predictive Accuracy and Fairness

```{r}
age_fairf1 = setMeasurePars(fairf1, grouping = function(df) as.factor(df$age > 30))
axgb = AutoxgboostMC$new(train.task, measures = list(acc, age_fairf1))
axgb$tune_threshold = FALSE
axgb$fit(time_budget = 30L)
p = axgb$predict(test.task)
```



## Scenario 4: Interpretability,  Predictive Accuracy and Robustness

```{r}
axgb = AutoxgboostMC$new(train.task, measures = list(auc, interpnf, robustnoise))
axgb$fit(time_budget = 15L)
p = axgb$predict(iris.task)

axgb$optim.result$pareto.front
axgb$optim.result$pareto.set
axgb$optim.result$pareto.inds
```

## Scenario 5: Optimizing a subjective measure.

In this scenario, we aim to find a model, that optimizes a measure we can not compute directly from the data, as it heavily depends of our current judgement of the proposed models. This might be a measure that can for example be described as follows:
"I want a model that achieves a very low false positive rate. It needs to be interpretable and fair with respect to a certain characteristic (i.e. the variable race).
Additionally, after looking at diagnosis plots, I can determine, that the relationships my model learns do correspond to the true underlying process,and thus provide a numerical rating for each model."

As a user usually can not look at all intermediate results, we might also want to learn user preferences from a few ratings given by the user and use this data to extrapolate to unrated models, only querying the user for new ratings once in a while.



## Setting Hyperparameters:

```{r}
axgb = AutoxgboostMC$new(train.task, measures = list(auc, timepredict))
axgb$set_hyperpars(list("early_stopping_rounds" = 25L))
axgb$early_stopping_rounds = 5
```


## List of available measures:

### Predictive Performance

A host of measures for predictive performance is available from the package `mlr`.
See `mlr::listMeasures()` or the {mlr Tutorial](https://mlr.mlr-org.com/) for a full list.

### Interpretability
Several measures for interpretability have been defined in [Molnar et al., 2019](https://arxiv.org/abs/1904.03867).
We implement those measures the following measures:

- Curve Complexity

```{r}
# Curve Complexity
interpmec
```

- Interaction Strength

```{r}
#' Interaction Strength
interpias
```

- Number of features

```{r}
# Number of features
interpnf
```

### Fairness

Fairness can be measured with respect to multiple criteria. In our case, we measure difference
with respect to a variable that groups the observational data into 2 groups.

1. Independence

```{r}
# Absolute differences of Positive Rate between groups
fairpr
```


2. Sufficiency

```{r}
# Absolute differences of F1 Scores between groups
fairf1
```

3. Calibration

```{r}
# Absolute differences of Positive Predictive Value between groups
fairf1
```

### Robustness

Robustness can be assessed with respect to multiple criteria. The most important ones incldue:

1. Corruption Robustness

See `?robustnoise` for more info

```{r}
# Robustness to feature corruption
robustnoise

# Robustness to featurewise corruption
robustnoiseperfeat
```

2. Adversarial Robustness

```{r}
# Not implemented
```

3. Robustness against distribution shift

```{r}
# Not implemented
```

### Prediction Speed

Prediction and training speed can be measured from within mlr using `timepredict` and `timetrain`.





## FIXME: UI Design Speccs

Tab1: AutoML Process:
Choose a budget, start the process, visualize progress wrt. some measures

Tab2: Pareto Front

Tab 3: Dataset overview

App: Starts / Controls and Visualizes results
