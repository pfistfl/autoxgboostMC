% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/AutoxgboostMC.R
\docType{data}
\name{AutoxgboostMC}
\alias{AutoxgboostMC}
\title{Fit and optimize a xgboost model for multiple criteria}
\format{An object of class \code{R6ClassGenerator} of length 24.}
\usage{
AutoxgboostMC
}
\arguments{
\item{task}{[\code{\link[mlr]{Task}}]\cr
The task to be trained.}

\item{measures}{[list of \code{\link[mlr]{Measure}}]\cr
Performance measure. If \code{NULL} \code{\link[mlr]{getDefaultMeasure}} is used.}

\item{parset}{[\code{\link[ParamHelpers]{ParamSet}}]\cr
Parameter set to tune over. Default is \code{\link{autoxgbparset}}.
Can be updated using `.$set_parset()`.}

\item{nthread}{[integer(1)]\cr
  Number of cores to use.
  If \code{NULL} (default), xgboost will determine internally how many cores to use.
  Can be set using `.$set_nthread()`.

Arguments to `.$fit()`:}

\item{iterations}{[\code{integer(1L}]\cr
Number of MBO iterations to do. Will be ignored if a custom \code{MBOControl} is used.
Default is \code{160}.}

\item{time_budget}{[\code{integer(1L}]\cr
Time that can be used for tuning (in seconds). Will be ignored if a custom \code{control} is used.
Default is \code{3600}, i.e., one hour.}

\item{fit_final_model}{[\code{logical(1)}]\cr
Should the model with the best found configuration be refitted on the complete dataset?
Default is \code{FALSE}. The model can also be fitted after optimization using `.$fit_final_model()`.}

\item{plot}{[\code{logical(1)}]\cr
  Should the progress be plotted? Default is \code{TRUE}.

The optimization process can be controlled via additional arguments to `.$optimizer`.
See `\code{\link{AxgbOptimizer}} for more information.

Additional arguments that control the Pipeline:}

\item{early_stopping_measure}{[\code{\link[mlr]{Measure}}]\cr
Performance measure used for early stopping. Picks the first measure
defined in measures by default.}

\item{early_stopping_rounds}{[\code{integer(1L}]\cr
After how many iterations without an improvement in the boosting OOB error should be stopped?
Default is \code{10}.}

\item{early_stopping_fraction}{[\code{numeric(1)}]\cr
What fraction of the data should be used for early stopping (i.e. as a validation set).
Default is \code{4/5}.}

\item{impact_encoding_boundary}{[\code{integer(1)}]\cr
Defines the threshold on how factor variables are handled. Factors with more levels than the \code{"impact_encoding_boundary"} get impact encoded while factor variables with less or equal levels than the \code{"impact_encoding_boundary"} get dummy encoded.
For \code{impact_encoding_boundary = 0L}, all factor variables get impact encoded while for \code{impact_encoding_boundary = .Machine$integer.max}, all of them get dummy encoded.
Default is \code{10}.}

\item{tune_threshold}{[logical(1)]\cr
Should thresholds be tuned? This has only an effect for classification, see \code{\link[mlr]{tuneThreshold}}.
Default is \code{TRUE}.}

\item{max_nrounds}{[\code{integer(1)}]\cr
Maximum number of allowed boosting iterations. Default is \code{3000}.}
}
\description{
An xgboost model is optimized based on a set of measures (see [\code{\link[mlr]{Measure}}]).
The bounds of the parameter in which the model is optimized, are defined by \code{\link{autoxgbparset}}.
For the optimization itself Bayesian Optimization with \pkg{mlrMBO} is used.
Without any specification of the control object, the optimizer runs for for 160 iterations or 1 hour,
whichever happens first.
Both the parameter set and the control object can be set by the user.

Arguments to `.$new()`:
}
\examples{
\donttest{
# Create a mlr Task
iris.task = makeClassifTask(data = iris, target = "Species")
# Instantiate the AutoxgboostMC Object
axgb = AutoxgboostMC$new(iris.task, measure = auc)
# Fit and Predict
axgb$fit(time_budget = 5L)
p = axgb$predict(iris.task)

# Set hyperparameters:
axgb$tune_threshold = FALSE
}
}
\keyword{datasets}
