% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/AutoxgboostMC.R
\docType{data}
\name{AutoxgboostMC}
\alias{AutoxgboostMC}
\title{Fit and optimize a xgboost model pipeline for multiple criteria}
\format{[R6::R6Class]}
\arguments{
\item{task}{[\code{\link[mlr]{Task}}]\cr
The task to be trained.}

\item{measures}{[list of \code{\link[mlr]{Measure}}]\cr
Performance measure. If \code{NULL} \code{\link[mlr]{getDefaultMeasure}} is used.}

\item{parset}{[\code{\link[ParamHelpers]{ParamSet}}]\cr
Parameter set to tune over. Default is \code{\link{autoxgbparset}}.
Can be updated using `.$set_parset()`.}

\item{nthread}{[integer(1)]\cr
Number of cores to use.
If \code{NULL} (default), xgboost will determine internally how many cores to use.
Can be set using `.$set_nthread()`.}

\item{pipeline}{[R6Class:AxgbPipeline(1)]\cr
The pipeline to optimize over. See `?AxgbPipelineXGB` for more info.
The pipeline can either be exchanged for a user-defined pipeline, or
adjusted via setting the hyperparams to `.$pipeline`.
Defaults to `AxgbPipeline$new()`.}

\item{optimizer}{[R6Class:AxgbOptimizer]\cr
The optimizer used for optimizing the pipeline.
The optimizer can either be exchanged for a user-defined optimizer, or
adjusted via setting the hyperparams to `.$optimizer`.
Defaults to `AxgbOptimizerSMBO$new()`.}

\item{iterations}{[\code{integer(1L}]\cr
Number of MBO iterations to do. Will be ignored if a custom \code{MBOControl} is used.
Default is \code{160}.}

\item{time_budget}{[\code{integer(1L}]\cr
Time that can be used for tuning (in seconds). Will be ignored if a custom \code{control} is used.
Default is \code{3600}, i.e., one hour.}

\item{fit_final_model}{[\code{logical(1)}]\cr
Should the model with the best found configuration be refitted on the complete dataset?
Default is \code{FALSE}. The model can also be fitted after optimization using `.$fit_final_model()`.}

\item{plot}{[\code{logical(1)}]\cr
  Should the progress be plotted? Default is \code{TRUE}.

* `.$fit_final_model()`: \cr
* `.$set_parset_bounds()`: \cr
* `.$get_opt_path_df()`: \cr
* `.$parset()`: \cr
* `.$logger()`: \cr}
}
\description{
An xgboost modeling pipeline is optimized based on a set of measures (see [\code{\link[mlr]{Measure}}]).
The bounds of the parameter in which the model is optimized, are defined by \code{\link{autoxgbparset}},
and can be adapted throught the learning process.
For the optimization itself Bayesian Optimization with \pkg{mlrMBO} is used.
Without any specification of the control object, the optimizer runs for for 160 iterations or 1 hour,
whichever happens first.
Both the parameter set and the control object can be set by the user.
}
\section{Construction}{
 \cr
 ```
 axgb = AutoxgboostMC$new(task, measure = list(auc, acc))
 ```
}

\section{Methods}{

* `.$fit()`: \cr
}

\section{Plot Methods}{

* `.$plot_pareto_front()`: \cr
* `.$plot_pareto_front_projections()`: \cr
* `.$plot_parallel_coordinates()`: \cr
* `.$plot_opt_path()`: \cr
* `.$set_parset_bounds()`: \cr

The optimization process can be controlled via additional arguments to `.$optimizer`.
See `\code{\link{AxgbOptimizer}}` for more information.
}

\examples{
\donttest{
# Create a mlr Task
iris.task = makeClassifTask(data = iris, target = "Species")
# Instantiate the AutoxgboostMC Object
axgb = AutoxgboostMC$new(iris.task, measure = auc)
# Fit and Predict
axgb$fit(time_budget = 5L)
p = axgb$predict(iris.task)

# Set hyperparameters:
axgb$tune_threshold = FALSE
}
}
\keyword{datasets}
